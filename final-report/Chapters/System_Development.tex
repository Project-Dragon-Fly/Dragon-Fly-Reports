\chapter{System Development}


\section{System Architecture}

A hybrid microservice architecture is utilized to gain the scalability and decouple AI engine and services to serve web request efficiently, while ensuring quicker and simpler development and deployment. Each independent services are created as a separate modular microservice which can be integrated based on requirement and infrastructure availability.
Client side request are sent handled by the system with nginx serving as a gateway \& reverse proxy. TLS/SSL encryption is added when accessing system outside trusted LAN network. 

When scaling up the system to handle large number of requests, Redis is used to provide caching, and as a additional faster volatile in-memory database. Message Queue is implemented using RabbitMQ, to utlise second workflow pipleline to allow large number of camera network to be tasked for processing in limited hardware infrastructure.


\subsection{CCTV Infrastructure}

\subsubsection{Existing CCTV Infrastructure}
Modern Infrastructure of CCTV (Closed-Circuit Television) Video surveillance system pan out to a Network connected IP camera, which is streamed to an NVR (Network Video Recorder). Popular streaming protocols like RTSP (Realtime Streaming Protocol), RTSPS (RTSP over SSL), RTMP (Real-Time Messaging Protocol) are utilized in most IP cameras and NVR. 

NVR stores the footage and is usually removed after specified period based on the storage capacity of the NVR and number of cameras connected. On most NVR firmware, playback and downloading of recording is only provided by builtin web interface, or proprietary softwares bundled, which are known to be poorly implemented. Some NVR interface only works with full features on outdated Internet Explorer after installing plug-in.

Live streams of NVR and IP cameras can be accessed easily as RSTP stream using the network ip address, which can be played by any client in the same network.


\subsubsection{CICET network}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=\linewidth]{Images/camera-network}
	\caption{CICET camera network}
	\label{fig:camera-network}
\end{figure}

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.32\linewidth]{Images/camera_footage/footage1} \hfill
	\includegraphics[width=0.32\linewidth]{Images/camera_footage/footage2} \hfill
	%	\includegraphics[width=0.32\linewidth]{Images/camera_footage/footage3} \\
	%	\vspace{3mm}
	%	\includegraphics[width=0.32\linewidth]{Images/camera_footage/footage4} \hfill
	\includegraphics[width=0.32\linewidth]{Images/camera_footage/night1} \hfill
	%	\includegraphics[width=0.32\linewidth]{Images/camera_footage/night2}
	\caption{Sample camera footage}
\end{figure}

The camera feed is obtained from a project deployed by Center for Information, Communication and Educational Technology (CICET), Government College of Engineering Kannur. Under the project entitled "Third eye", a virtual network was established covering an area of 350 $KM^2$, connecting 9 institutes. Using the optical fiber network and wireless network, various camera are installed allowing easy and secure storage and streaming. The network is isolated using dedicated VLAN (Virtual Local Area Network), and currently holds about 190 cameras. Access to these footage are obtained by secured virtual private network. 
The system utlised Uniview (UNV) camera and NVR system, with each NVR having storage to maintain retention of 15 days. Only RSTP streaming was accessible, with resolution upto 1440p (most camera were set at 1080p) at 25FPS. IP camera were integrated with onboard microSD card of 64GB or 128GB to faciliate as backup buffer when the network connection was down.


\subsubsection{VPN based Secure network access}
CCTV Video surveillance systems utilize restricted closed network isolated from the internet, to reduce Cyber attack vectors and unauthorized usage.  VPN (Virtual Private Network) split tunnel enables secure communication interface to the network over unsecured channels. Wireguard, free and open source communication protocol distributed in Linux Kernel implementing encrupted VPN, is utilized for the creation of VPN interface with very less overhead and very high performance. 

\subsection{Video Stream Ingestion}
Popular streaming protocols like RTSP (Realtime Streaming Protocol), RTSPS (RTSP over SSL), RTMP (Real-Time Messaging Protocol) are utilized in most IP cameras and NVR. 
Video stream from NVR and IP camera are ingested by Stream Handler and FrameGen module for processing using OpenCV. FrameGen module reads the live stream of camera and save frames to storage backend with timestamp and signature as metadata. It ignores dead frames, blacklisted and idle frames, saving only required frames thereby reducing storage requirement. FrameGen Module is launched as a cron job using scheduled Celery Beat task, configured to process specified list of camera for specified duration regularly. 

Additionally, FFmpeg is used for on-the-fly transcoding (transcoding while streaming) of live RSTP stream (which is a pull stream) from IP camera and NVR, to RTMP stream (which is a push stream) to NGINX Media Server. A stream key is attached to each stream which NGINX relays as HLS (HTTP Live Streaming) securely over SSL (Secure Socket Layer) to client browser of the authenticated user containing stream key.   

\subsection{Control \& Management Backend}
Control and Management Backend is a Django based Monolith application containing User Authentication, Business Logic, and Presentation layer logic. It handles user requests, and initiates corresponding pipeline or trigger micro-service process. It also serves as the interface to primary PostgreSQL relational database, through and Object Relation Mapping (ORM) layer in Django, handling all DB queries and processing business logic.

\begin{figure}[!ht]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/architecture_high_level}
	\caption{High level system architecture}
	\label{fig:architecturehighlevel}
\end{figure}

When a user launches the app, the request is processed by middlewares in django, checking security, user authenticity and other pre-processing tasks. The sanitized request is handled by the corresponding view that processes the business logic, and renders the template and returns the response to user.

\subsubsection{Load Balancer \& Reverse Proxy}
NGINX is used as a load balancer, gateway and Reverse proxy. It handles request from client, preferably encrypted over SSL, proxies to corresponding upstream server or application server. It directly serves static files.

\subsubsection{Media Streaming Server}
Media Streaming Server is used to provide secure live stream camera footage access to the user using HLS (HTTP Live Streaming) over SSL. It relays the stream received to the client. Nginx with RTMP module is used as a Media Streaming Server. Alternative include Kurento Media Server, Ant Media Server etc.

\subsubsection{AI Services}
AI Detection Engines are wrapped into service packages exposing APIs like REST API, WebSocket etc. Being an independent micro-services requiring web interface, FastAPI is used as a lightweight web framework to process HTTP \& WS request and perform the corresponding business logic which includes the prediction by the detection engine, retrieving class labels, accuracy values etc. As detection and processing completes, responses are pushed to the client using WebSocket connection and saved to the database.

\subsubsection{Datastore}
Data store includes storage backend and databases. 
	\begin{enumerate}
		\item Database \\
		Primary database used is PostgreSQL relational database. All the configuration details, user details, authentication information are stored in this database. Database is accessed using ORM, enabling modeling of real world information using objects which are stored as relation in SQL tables.
		
		\item Storage Backend \\
		For local development setup, Linux File system is used as storage backend. For production grade setup MinIO Server is used as the storage backend to provide secure redundancy, segregated access, and hybrid multi-tenant enterprise grade system. Each file is identified by the path and object id and can be retrieved by authorized users.
		
		Video Files, Frames, Snapshots, thumbnails and other static files are organized and storage in persistent storage backend. Public static files are served directly through NGINX.
	\end{enumerate}

	


\subsection{Camera Feed Live Streaming}
\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.8\linewidth]{Images/live_stream_arch}
	\caption{Camera Feed Live Streaming}
	\label{fig:livestreamarch}
\end{figure}
Initially, WireGuard split tunnel is created and connection to CCTV network is established. Then FFmpeg instances are initiated as celery task, streaming keys and camera details passed as arguments. The FFmpeg pulls RSTP (x265 HEVC) video stream from the camera network, transcodes on-the-fly to RTMP (flv, x264) video stream, which is pushed to Nginx media server live ingress. Nginx relays the transcoded video stream to browser client using HLS (if SSL is configured, HLS over TSL/SSL) requesting the correct secret key. This enables secure streaming of video efficiently. The transcoding proccess is CPU intensive proccess, requiring higher end servers to real-time transcoding of multiple streams.

\subsubsection*{Challenges}
\begin{itemize}
	\item Sharing streaming key among multiple users, would require same key to be used for extended period of time, which may result in leak of stream key, and access by unauthorized users
	\item Using dedicated streams for each user by FFmpeg, would require multiple instances of FFmpeg transcoding resulting in significant CPU usage for redundant proccess
\end{itemize}
The solution of the challenges would be separating transcoding procces, and streaming process. Using same transcoded video for all the streams. Using specialised media servers like Ant Media Server, and Kurento Media Server, instead of Nginx RTMP module, would lead to better performance and more builtin features and support, including ultra-low latency streaming using WebRTC 

\section{Workflow Pipeline}

\begin{figure}[ht!]
	\centering
	\begin{subfigure}[b]{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/pipeline1}
		\caption{pipeline1}
		\label{fig:pipeline1}
	\end{subfigure}
	
	\begin{subfigure}[b]{0.8\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/pipeline2}
		\caption{pipeline2}
		\label{fig:pipeline2}
	\end{subfigure}
	\caption{Workflow pipeline}
\end{figure}

\subsection*{Pipeline 1}
In this mode of operation, all camera assigned will have a dedicated AI engine instance (or kubernetes pod) running continuously. The results along with necessary meta data like timestamps, are saved into the database. When user queries a description, query is processed to match features from the database, and route is computed. 

This is costly in terms of resource requirements as each camera will have a dedicated AI engine instance, requiring significant amount of computational power. Without Nvidia MPS (Multi Process Service) or GPU partitioning, number of GPU's required may result in significant expense.

\subsection*{Pipeline 2}
In this mode of operation, cameras are set to \textit{ON\_DEMAND proccessing mode}. When a user queries a description, the system first check if the matching camera is processed or not. If it is already processed data is taken from database otherwise the camera processing task is pushed to Job queue. The consumer regularly consumes from Job Queue, when hardware resources are free. When Queue is consumed, the a new detection engine instance is launched, and the detection task is initiated and results are stored in the database. When task is completed, the results are fetched from user, the whole system would be dead.

This is much less resource intensive but requires complex infrastructure. Queue helps in load balancing, and allows usage of less GPUs and servers to process large number of camera networks.





\section{UI Design}
User experience is a critical function to realize potential of any system, so an elegant minimalistic design is used to develop the system. Screenshots of the User Interface design is provide in figure \ref{fig:home}.

User can search queries through the home page in simple words, on submitting the query, user is treated with a list of detection, of which the user selects the most optimal choice, the shortlist selection process continues. The route generated is displayed in real-time in the route map. The camera live stream page shows the authorized camera feed accessible by the user.
\begin{figure}[!ht]
	\centering
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/UI/home}
		\caption{Home page}
		\label{fig:home}
	\end{subfigure} \hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/UI/login}
		\caption{Login page}
		\label{fig:login}
	\end{subfigure} \\ \vspace{3mm}
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/UI/result}
		\caption{Result}
		\label{fig:result1}
	\end{subfigure} \hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/UI/live_stream}
		\caption{Camera Live Stream}
		\label{fig:result2}
	\end{subfigure} \hfill
	\begin{subfigure}[b]{0.48\linewidth}
		\centering
		\includegraphics[width=\linewidth]{Images/UI/tracing}
		\caption{Tracing}
		\label{fig:tracing}
	\end{subfigure}
	\caption{UI Design}
\end{figure}

\section{AI Detection Engine}
\subsection{YOLOv4}
YOLOv4 model is used for the detection of vehicles. Transfer learning was conducted on the AI model that significantly reduced the training time required. Transfer learning is the process of using a pre-trained model and changing its parameter to learn new/fine tuned concept. Pre-trained weight was extracted from the official YOLOv4 repository \cite{darknet} that used MS COCO dataset to train 80 classes. Parameter tuning is performed which are detailed in table \ref{tab:yolo_parameter}.

\begin{table}[!ht]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		Input size            & $416 * 416$  \\ \hline
		Input Channels        & 3            \\ \hline
		Batch size            & 32           \\ \hline
		learning rate         & 0.0013       \\ \hline
		Yolo layer            & $[256*256],[512*512],[1024*1024]$ \\ \hline
		Total Layers          & 162          \\ \hline
		Target Classes (9)    & \begin{tabular}[c]{@{}l@{}}
			auto, bus, tempo traveler, tractor, \\
			truck, van, two wheeler, car, jcb\end{tabular} \\ \hline
	\end{tabular}%
	\caption{YOLOv4 Parameter}
	\label{tab:yolo_parameter}
\end{table}

The model was trained using Darknet at Google Colab. Later the environment was changed to the Workstation provided by the Dept. of Computer Science and Engineering, Govt. College of Engineering Kannur. A total of approx. 48 hours are spend in training, reaching 97.7\% mean Average Precision (mAP) at 7300 iterations. Figure \ref{fig:darknettrainingchart} summarizes training.

The dataset for training was collected at two different sources. Kaggle provided a wide variety of Indian Vehicle dataset that spanned across country. Footage from CICET camera network is also extracted. Each and every frame was labeled using LabelImg tool. Summary of the labeled dataset is depicted in the table \ref{tab:dataset_sum1}.

\begin{table}[!ht]
	\begin{tabular}{m{0.45\linewidth} m{0.45\linewidth}}
%		\begin{figure}
			\centering
			\includegraphics[width=\linewidth]{Images/darknet_training_chart}
			\captionof{figure}{YOLOv4 training chart}
			\label{fig:darknettrainingchart}			
%		\end{figure}

		 & 
		 
		 \begin{tabular}{|l|c|c||c|}
		 	\hline
		 	\textbf{Class}        & \textbf{Kaggle} & \textbf{CICET} & \textbf{Total} \\ \hline
		 	Two wheeler           & 557             & 291            & \textbf{848}   \\ \hline
		 	Truck                 & 354             & 170            & \textbf{424}   \\ \hline
		 	Auto                  & 297             & 146            & \textbf{443}   \\ \hline
		 	car                   & 233             & 432            & \textbf{665}   \\ \hline
		 	bus                   & 220             & 112            & \textbf{332}   \\ \hline
		 	tractor               & 133             & 0              & \textbf{113}   \\ \hline
		 	van                   & 101             & 131            & \textbf{232}   \\ \hline
		 	JCB                   & 1               & 0              & \textbf{1}     \\ \hline \hline
		 	\textbf{Total Boxes}  & \textbf{1956}   & \textbf{1282}  & \textbf{3238}  \\ \hline
		 	\textbf{Total Images} & \textbf{733}    & \textbf{826}   & \textbf{1559}  \\ \hline
		 \end{tabular}
		 \captionof{table}{Dataset summary}
		 \label{tab:dataset_sum1}
	\end{tabular}
\end{table}

\subsection{DeepSORT}
DeepSORT is utilized to assign unique id to each object in an continuous frame. The implementation is directly extracted from the article of AIGuysCode \cite{theaiguyscode_deepsort}. The given model tries to extract human features as the deep matrix. No modification are made as Kalman filter provides greater accuracy. However, learning the deep matrix of vehicles can further increase accuracy and aid in siamese network.

\subsection*{Image Dictionary}
In-order to facilitate correct labeling of images, a concept called Image Dictionary is introduced. Image Dictionary is a hierarchical classification of images. Each type can have its sub-type. A visual representation is also provided for each type. It is build using D3 JS.
\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{Images/image_dictionary}
	\caption{Image Dictionary}
\end{figure}


\section{Algorithm}
\begin{breakablealgorithm}
	\caption{Process and store detections $\forall$ camera}
	~~ \\\textbf{INPUT} : $video$, $camProp$
	\\ \textbf{OUTPUT} : $id$, $class$, $bbox$, $time$, $featureTensor$ $\rightarrow$ $Database$
	\begin{algorithmic}[1]
		\State $FPS \gets video.FPS$
		\State $time \gets camProp.startTime$
		\State $tracker \gets \Call{Tracker}{cosineMetric}$
		\ForAll{$frame \in video.frames$}
			\State $names, bboxes, scores \gets \Call{YOLOv4}{frame}$
			\State $features \gets \Call{encoder}{frame,bboxes}$ \Comment{feature extraction}
			\State $detections \gets []$
			\For{$i \in \Call{lenght}{names}$}
				\State $detections[i] \gets (names[i],bboxes[i],scores[i],features[i])$
			\EndFor
			\State $\Call{tracker.predict}{\space}$
			\State $\Call{tracker.update}{detections}$ \Comment{DeepSORT assigns id}
			
			\ForAll{$track \in tracker.tracks$}
				\If{$not \Call{track.isConfirmed}{\space}$} \Comment{Skip first few frame to ensure objects presence}
					\State Continue
				\ElsIf{$track.time_since_update \ge 1$} \Comment{Missing object in current frame}
					\State Continue
				\EndIf
				\State $\Call{Database.Write}{track.id, track.class, track.tbwh, time, track.feature}$
			\EndFor
			\State $time \gets time + \frac{1}{FPS}$
		\EndFor
	\end{algorithmic}
\end{breakablealgorithm}

\begin{breakablealgorithm}
	\caption{Route building}
	~~ \\\textbf{INPUT} : $vehicleClass$, $initialCamera$, $startTime$, $endTime$
	\\ \textbf{OUTPUT} : $route$
	\begin{algorithmic}[1]	
		\Function{filterVehicle}{vehicleList,currentCam,startTime,endTime} \Comment{Support function}
			\State $DB \gets currentCam.DataBase$
			\State $DB \gets \Call{DB.filter}{name=vehicleClass}$
			\State $fDB \gets \Call{DB.filter}{time \geq startTime}$
			\State $fDB \gets \Call{fDB.filter}{time \leq endTime}$
			
			\ForAll{ $vid \in \Call{uniqueID}{fDB}$}
				\State $vDB \gets \Call{DB.filter}{id = vid}$
				\State $enterTime \gets \Call{min}{vDB.time}$
				\State $exitTime \gets \Call{max}{vDB.time}$
				\State $\Call{vehicleList.append}{vid,enterTime,exitTime,currentCam}$
			\EndFor
		\EndFunction
		
		\State $route \gets []$
		\State $vehicleList \gets []$
		\State \Call{filterVehicle}{vehicleList,initialCamera,startTime,endTime} \Comment{Parse camera}
			
		
		\While{$\Call{count}{vehicleList} \ge 0 $ AND $\Call{userInterrupt}{\space} == FALSE$}
			\If{$\Call{count}{vehicleList} == 1$}
				\State $seletedVehicle \gets vehicleList[0]$
			\Else
				\State $selectedVehicle \gets \Call{UserSelect}{vehicleList}$ \Comment{Prompt user interaction}
			\EndIf
			\State $\Call{route.append}{selectedVehicle}$ \Comment{Add to route list}
			
			\State $addedCam \gets selectedVehicle.camera$
			\State $nextCameras \gets \Call{addedCam.neighbor}{radius=1KM}$
			\State $vehicleList \gets []$
			
			\For{$currentCam \in nextCameras$} \Comment{Loop surrounding camera}
				\State $startTime \gets selectedVehicle.exitTime + \Call{minTravelTime}{addedCam,currentCam}$
				\State $endTime \gets selectedVehicle.exitTime + \Call{maxTravelTime}{addedCam,currentCam}$
				
				\State \Call{filterVehicle}{vehicleList,currentCam,startTime,endTime} \Comment{Parse camera}
			\EndFor
		\EndWhile
		\State \Call{print}{route} \Comment{The final result}
	\end{algorithmic}
\end{breakablealgorithm}

\section{Technology Stack}
Development is primarily done in Python, while JavaScript is utilized for front-end scripting.

The list of frameworks and libraries used represent those which are widely utilized or have significant impact in the development.

\subsubsection*{Languages}
\begin{itemize}
	\item Python (Primary Language): Used in all server-side code.
	\item JavaScript: Used in front-end Scripting, WebSocket Browser Client, OpenPlayerJS
\end{itemize}

\subsubsection*{Frameworks}
\begin{itemize}
	\item Django: Back-end Server
	\item FastAPI: API web framework for micro-service like AI Engine and provides WebSocket Abstraction
	\item Keras \& TensorFlow: AI inferencing
	\item Darknet \& CUDA: Training Model
\end{itemize}

\subsubsection*{Other Tools}
\begin{itemize}
	\item Nginx: Used as web server and reverse proxy for handling request, serving static files, proxying request to application servers.
	\item Nginx RTMP Module: Used to enhance Nginx to be used as a Media Streaming Server
	\item PostgreSQL: As Relational Database
	\item WireGaurd: Creating secured encrypted VPN tunnel to NVR infrastructure network
	\item FFmpeg: For on-the-fly video transcoding to relay Live Camera RSTP feed to nginx as RTMP ingress feed
	\item OpenCV: Image Processing in FrameGen module and handling camera feed.
	\item WebSocket: Bidirectional server-push communication with user
	\item Gunicorn: WSGI Application server to handle django WSGI application.
	\item Uvicorn: ASGI Application server to handle FastAPI and other ASGI applications. 
	\item OpenPlayerJS: Browser-based HLS video stream playback
	\item MinIO: File and object storage back-end to replicate Amazon Web Service S3.		%NI
	\item Docker and Docker-Compose: For containerisation of services
	\item GitHub Action: For Continous Integration and Deployment (CI/CD) pipleline
	\item Crontab: For scheduled execution of programs
	\item Celery: Asynchronous task queue, and peroidic task schduler
	\item RabbitMQ: Message Broker and Queue
	\item Redis: Used for providing caching support and in-memory database
	\item Sentry: For application and Performance monitoring
\end{itemize}